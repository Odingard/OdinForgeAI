import type { AgentMemory, AgentResult, ExploitFindings } from "./types";
import type OpenAI from "openai";
import { generateAdversaryPromptContext } from "./adversary-profile";
import { wrapAgentError } from "./error-classifier";
import { formatExecutionModeConstraints } from "./policy-context";
import { createExploitModelRouter } from "./model-router";
import { buildNetworkGroundTruth, buildReconGroundTruth } from "./scan-data-loader";
import { EXPLOIT_AGENT_TOOLS, executeExploitTool, type ToolCallEvidence, type ExploitToolContext } from "./exploit-tools";

/**
 * AGENTIC EXPLOIT AGENT
 *
 * Multi-turn tool-calling agent that reasons about attack vectors, invokes
 * real security validation tools (SQLi, XSS, SSRF, fuzzing, port scanning,
 * SSL analysis, protocol probes), analyzes results, and produces findings
 * backed by actual HTTP evidence.
 *
 * Execution mode gating (safe/simulation/live) controls which tools are
 * available. In safe mode the agent uses passive tools only.
 *
 * Supports model-agnostic LLM routing with optional XBOW-style alloy
 * rotation (multiple models within one conversation thread).
 *
 * Function signature unchanged — the orchestrator doesn't know the difference.
 */

type ProgressCallback = (stage: string, progress: number, message: string) => void;

const MAX_TURNS = 12;
const LOOP_TIMEOUT_MS = 110_000; // 110s — leaves 10s buffer within 120s circuit breaker

export async function runExploitAgent(
  memory: AgentMemory,
  onProgress?: ProgressCallback
): Promise<AgentResult<ExploitFindings>> {
  const startTime = Date.now();
  const executionMode = memory.context.executionMode || "safe";

  onProgress?.("exploit", 30, "Initializing agentic exploit analysis...");

  // ── Build prompts ──────────────────────────────────────────────────

  const reconContext = memory.recon
    ? `
Previous Recon Findings:
- Attack Surface: ${memory.recon.attackSurface.join(", ")}
- Entry Points: ${memory.recon.entryPoints.join(", ")}
- API Endpoints: ${memory.recon.apiEndpoints.join(", ")}
- Auth Mechanisms: ${memory.recon.authMechanisms.join(", ")}
- Technologies: ${memory.recon.technologies.join(", ")}
- Potential Vulnerabilities: ${memory.recon.potentialVulnerabilities.join(", ")}
`
    : "";

  const adversaryContext = memory.context.adversaryProfile
    ? generateAdversaryPromptContext(memory.context.adversaryProfile)
    : "";

  const policyContext = memory.context.policyContext || "";
  const executionModeConstraints = formatExecutionModeConstraints(executionMode);

  const groundTruthContext = memory.groundTruth
    ? [buildNetworkGroundTruth(memory.groundTruth), buildReconGroundTruth(memory.groundTruth)]
        .filter(Boolean)
        .join("\n\n")
    : "";

  const systemPrompt = `You are the EXPLOIT AGENT, a specialized AI exploitation analysis system for OdinForge AI.

You have access to real security testing tools. Your workflow:
1. Analyze ground-truth data and recon findings to understand the target
2. Identify highest-priority attack vectors
3. Use tools to validate — start with most impactful tests
4. Chain findings: SQLi confirmed → check data exfil; SSRF found → probe cloud metadata
5. After sufficient testing (or when tools are exhausted), produce your final JSON analysis

RULES:
- You are in ${executionMode} mode. Some tools may be blocked — adapt accordingly.
- Do NOT test the same endpoint + vulnerability type twice.
- When a tool confirms a vulnerability, note it and move to chaining/escalation.
- If active tools are blocked (safe mode), use passive tools (http_fingerprint, port_scan, check_ssl_tls, run_protocol_probe) and produce analytical assessment augmented with passive recon data.
- Think like an offensive security expert. Be realistic about success likelihood.
- Maximize the value of each tool call — don't waste turns on redundant probes.
${adversaryContext}
${executionModeConstraints}
${policyContext}`;

  const userPrompt = `Analyze exploitability for this security exposure:

Asset ID: ${memory.context.assetId}
Exposure Type: ${memory.context.exposureType}
Priority: ${memory.context.priority}
Description: ${memory.context.description}
${reconContext}
${groundTruthContext ? `\n${groundTruthContext}\n` : ""}
Begin by calling the most relevant tool(s) to validate the highest-risk attack vectors, then analyze results and continue probing as needed.`;

  const finalTurnInstruction = `You have finished your investigation. Now produce your final exploitation analysis as a JSON object with this exact structure (nothing else):
{
  "exploitable": boolean,
  "exploitChains": [
    {
      "name": "Name of the exploit chain",
      "technique": "MITRE ATT&CK technique ID (e.g., T1190)",
      "description": "Detailed description including evidence from tool results",
      "success_likelihood": "high" | "medium" | "low"
    }
  ],
  "cveReferences": ["CVE-XXXX-XXXX if applicable"],
  "misconfigurations": ["list of exploitable misconfigurations"]
}

CRITICAL RULES:
- If ANY tool returned vulnerable=true or confidence>=50, you MUST include at least one exploit chain. Set exploitable=true.
- Every confirmed vulnerability from tool results MUST appear as an exploit chain with the specific vulnerability type in the name (e.g., "SQL Injection via email parameter", "XSS in comment field", "Path Traversal via null byte").
- Include the tool name and key evidence details in each chain's description.
- Do not omit findings just because you didn't have time to fully chain them. A single confirmed vulnerability is still an exploit chain.`;

  // ── Agentic loop ───────────────────────────────────────────────────

  const router = createExploitModelRouter();
  const toolCtx: ExploitToolContext = {
    executionMode,
    organizationId: memory.context.organizationId,
    evaluationId: memory.context.evaluationId,
    assetId: memory.context.assetId,
  };

  const messages: OpenAI.ChatCompletionMessageParam[] = [
    { role: "system", content: systemPrompt },
    { role: "user", content: userPrompt },
  ];

  const allEvidence: ToolCallEvidence[] = [];
  const toolCallLog: NonNullable<ExploitFindings["toolCallLog"]> = [];

  try {
    for (let turn = 0; turn < MAX_TURNS; turn++) {
      // Timeout guard
      if (Date.now() - startTime > LOOP_TIMEOUT_MS) {
        onProgress?.("exploit", 38, "Timeout approaching — finalizing...");
        break;
      }

      const isFinalTurn = turn === MAX_TURNS - 1;
      const progressPct = 30 + Math.floor((turn / MAX_TURNS) * 10);

      // Force final answer on last turn
      if (isFinalTurn) {
        messages.push({ role: "system", content: finalTurnInstruction });
      }

      const { client, model } = router.getForTurn(turn);

      onProgress?.(
        "exploit",
        progressPct,
        turn === 0
          ? "Analyzing attack vectors..."
          : `Turn ${turn + 1}: probing targets...`
      );

      const response = await client.chat.completions.create({
        model,
        messages,
        tools: isFinalTurn ? undefined : EXPLOIT_AGENT_TOOLS,
        tool_choice: isFinalTurn ? undefined : "auto",
        max_completion_tokens: 2048,
      });

      const choice = response.choices[0];
      if (!choice) {
        throw new Error("No response from Exploit Agent");
      }

      const assistantMsg = choice.message;
      messages.push(assistantMsg as OpenAI.ChatCompletionMessageParam);

      // No tool calls → LLM produced its final answer
      if (!assistantMsg.tool_calls || assistantMsg.tool_calls.length === 0) {
        break;
      }

      // Execute tool calls
      for (const toolCall of assistantMsg.tool_calls) {
        if (toolCall.type !== "function") continue;
        const fnName = toolCall.function.name;
        let fnArgs: Record<string, unknown> = {};
        try {
          fnArgs = JSON.parse(toolCall.function.arguments);
        } catch {
          // Malformed args — tell the model
          messages.push({
            role: "tool",
            tool_call_id: toolCall.id,
            content: JSON.stringify({ error: "Invalid JSON arguments" }),
          });
          continue;
        }

        onProgress?.(
          "exploit",
          progressPct + 1,
          `Executing ${fnName}...`
        );

        const { result, evidence } = await executeExploitTool(fnName, fnArgs, toolCtx);

        messages.push({
          role: "tool",
          tool_call_id: toolCall.id,
          content: result,
        });

        if (evidence) {
          allEvidence.push(evidence);
          toolCallLog.push({
            turn,
            toolName: evidence.toolName,
            arguments: evidence.arguments,
            resultSummary: evidence.resultSummary,
            vulnerable: evidence.vulnerable,
            confidence: evidence.confidence,
            executionTimeMs: evidence.executionTimeMs,
          });
        }
      }
    }

    // Force a final JSON-producing call if the last message isn't valid findings JSON.
    // This handles: (a) loop ended on a tool result, (b) loop ended on assistant
    // text that's a natural-language analysis rather than structured JSON.
    const lastMessage = messages[messages.length - 1];
    const lastIsToolResult = !lastMessage || (lastMessage as any).role === "tool";
    const lastIsAssistantWithPendingTools =
      (lastMessage as any)?.role === "assistant" && (lastMessage as any).tool_calls?.length > 0;
    const lastIsNonJsonAssistant = (() => {
      if ((lastMessage as any)?.role !== "assistant") return false;
      const content = (lastMessage as any).content;
      if (typeof content !== "string" || !content.trim()) return true;
      // Check if it parses as valid findings JSON
      try {
        const jsonMatch = content.match(/```(?:json)?\s*([\s\S]*?)```/) || [null, content];
        const parsed = JSON.parse(jsonMatch[1]!.trim());
        return !parsed || typeof parsed.exploitable === "undefined";
      } catch {
        return true; // Not JSON — needs final call
      }
    })();

    const needsFinalCall = lastIsToolResult || lastIsAssistantWithPendingTools || lastIsNonJsonAssistant;

    if (needsFinalCall && Date.now() - startTime < LOOP_TIMEOUT_MS) {
      messages.push({ role: "system", content: finalTurnInstruction });
      const { client, model } = router.getForTurn();
      const finalResponse = await client.chat.completions.create({
        model,
        messages,
        max_completion_tokens: 2048,
      });
      const finalMsg = finalResponse.choices[0]?.message;
      if (finalMsg) {
        messages.push(finalMsg as OpenAI.ChatCompletionMessageParam);
      }
    }

    onProgress?.("exploit", 40, "Parsing exploit findings...");

    // ── Extract findings from last assistant message ──────────────────

    const findings = extractFindings(messages);
    const enriched = enrichWithEvidence(findings, allEvidence, toolCallLog);

    return {
      success: true,
      findings: enriched,
      agentName: "Exploit Agent",
      processingTime: Date.now() - startTime,
    };
  } catch (error) {
    throw wrapAgentError("Exploit Agent", error);
  }
}

// ── Helpers ────────────────────────────────────────────────────────────

/** Extract ExploitFindings JSON from the last assistant message with content. */
function extractFindings(messages: OpenAI.ChatCompletionMessageParam[]): ExploitFindings {
  // Walk backwards to find last assistant message with text content
  for (let i = messages.length - 1; i >= 0; i--) {
    const msg = messages[i] as any;
    if (msg.role === "assistant" && typeof msg.content === "string" && msg.content.trim()) {
      const content = msg.content.trim();
      // Try to extract JSON from markdown code block or raw JSON
      const jsonMatch = content.match(/```(?:json)?\s*([\s\S]*?)```/) || [null, content];
      try {
        const parsed = JSON.parse(jsonMatch[1]!.trim()) as ExploitFindings;
        return validateFindings(parsed);
      } catch {
        // Try the full content
        try {
          const parsed = JSON.parse(content) as ExploitFindings;
          return validateFindings(parsed);
        } catch {
          // Can't parse — return empty findings
        }
      }
    }
  }

  return {
    exploitable: false,
    exploitChains: [],
    cveReferences: [],
    misconfigurations: [],
  };
}

/** Validate and normalize parsed findings. */
function validateFindings(raw: any): ExploitFindings {
  return {
    exploitable: Boolean(raw.exploitable),
    exploitChains: Array.isArray(raw.exploitChains)
      ? raw.exploitChains.map((chain: any) => ({
          name: String(chain.name || "Unknown"),
          technique: String(chain.technique || "T0000"),
          description: String(chain.description || ""),
          success_likelihood: validateLikelihood(chain.success_likelihood),
        }))
      : [],
    cveReferences: Array.isArray(raw.cveReferences) ? raw.cveReferences.map(String) : [],
    misconfigurations: Array.isArray(raw.misconfigurations) ? raw.misconfigurations.map(String) : [],
  };
}

function validateLikelihood(likelihood: unknown): "high" | "medium" | "low" {
  const valid = ["high", "medium", "low"];
  return valid.includes(String(likelihood)) ? (likelihood as "high" | "medium" | "low") : "medium";
}

/** Match tool evidence to exploit chains and attach validation data. */
function enrichWithEvidence(
  findings: ExploitFindings,
  evidence: ToolCallEvidence[],
  toolCallLog: NonNullable<ExploitFindings["toolCallLog"]>
): ExploitFindings {
  if (evidence.length === 0) {
    return { ...findings, toolCallLog: toolCallLog.length > 0 ? toolCallLog : undefined };
  }

  // Build lookup of confirmed vulnerabilities from evidence
  const confirmedEvidence = evidence.filter((e) => e.vulnerable);

  const enrichedChains = findings.exploitChains.map((chain) => {
    // Try to match evidence to this chain by vuln type keyword or tool name
    const chainText = `${chain.name} ${chain.description} ${chain.technique}`.toLowerCase();
    const matchingEvidence = confirmedEvidence.filter((e) => {
      const toolText = `${e.toolName} ${e.resultSummary} ${JSON.stringify(e.arguments)}`.toLowerCase();
      // Match by vulnerability type keywords
      const vulnTypes = ["sqli", "xss", "ssrf", "command_injection", "path_traversal", "auth_bypass", "fuzz", "ssl", "port", "smtp", "dns"];
      return vulnTypes.some((vt) => chainText.includes(vt) && toolText.includes(vt)) ||
        // Or match by URL/endpoint
        (e.arguments.url && chainText.includes(String(e.arguments.url).toLowerCase()));
    });

    if (matchingEvidence.length > 0) {
      const bestMatch = matchingEvidence.reduce((a, b) => (a.confidence > b.confidence ? a : b));
      return {
        ...chain,
        validated: true,
        validationVerdict: evidenceToVerdict(bestMatch.confidence),
        validationConfidence: bestMatch.confidence,
        evidence: matchingEvidence.map((e) => ({
          toolName: e.toolName,
          summary: e.resultSummary,
          request: e.httpEvidence?.request,
          response: e.httpEvidence?.response,
          timing: e.httpEvidence?.timing,
        })),
      };
    }

    return chain;
  });

  return {
    ...findings,
    exploitChains: enrichedChains,
    toolCallLog: toolCallLog.length > 0 ? toolCallLog : undefined,
  };
}

function evidenceToVerdict(confidence: number): "confirmed" | "likely" | "theoretical" | "false_positive" {
  if (confidence >= 80) return "confirmed";
  if (confidence >= 50) return "likely";
  if (confidence >= 20) return "theoretical";
  return "false_positive";
}

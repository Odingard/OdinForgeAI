import type { AgentMemory, AgentResult, ExploitFindings } from "./types";
import type OpenAI from "openai";
import { generateAdversaryPromptContext } from "./adversary-profile";
import { wrapAgentError } from "./error-classifier";
import { formatExecutionModeConstraints } from "./policy-context";
import { createExploitModelRouter } from "./model-router";
import { buildNetworkGroundTruth, buildReconGroundTruth } from "./scan-data-loader";
import { EXPLOIT_AGENT_TOOLS, executeExploitTool, type ToolCallEvidence, type ExploitToolContext } from "./exploit-tools";
import { formatExternalRecon } from "./recon";
import { AevTelemetryRecorder } from "../aev-telemetry";
import type { AevRunStopReason } from "@shared/schema";

/**
 * AGENTIC EXPLOIT AGENT
 *
 * Multi-turn tool-calling agent that reasons about attack vectors, invokes
 * real security validation tools (SQLi, XSS, SSRF, fuzzing, port scanning,
 * SSL analysis, protocol probes), analyzes results, and produces findings
 * backed by actual HTTP evidence.
 *
 * Execution mode gating (safe/simulation/live) controls which tools are
 * available. In safe mode the agent uses passive tools only.
 *
 * Supports model-agnostic LLM routing with optional XBOW-style alloy
 * rotation (multiple models within one conversation thread).
 *
 * Function signature unchanged — the orchestrator doesn't know the difference.
 */

type ProgressCallback = (stage: string, progress: number, message: string) => void;

const MAX_TURNS = 12;
const LOOP_TIMEOUT_MS = 110_000; // 110s — leaves 10s buffer within 120s circuit breaker
const RATE_LIMIT_RETRIES = 3;
const RATE_LIMIT_BACKOFF_MS = 5_000;

async function withRateLimitRetry<T>(fn: () => Promise<T>): Promise<T> {
  for (let attempt = 0; attempt < RATE_LIMIT_RETRIES; attempt++) {
    try {
      return await fn();
    } catch (err: any) {
      const isRateLimit = err?.status === 429 ||
        err?.code === "rate_limit_exceeded" ||
        err?.message?.includes("rate limit");
      if (!isRateLimit || attempt === RATE_LIMIT_RETRIES - 1) throw err;
      await new Promise((r) => setTimeout(r, RATE_LIMIT_BACKOFF_MS * (attempt + 1)));
    }
  }
  throw new Error("Rate limit retry exhausted");
}

export interface ExploitAgentOptions {
  /** When true, attach raw LLM messages to findings for flag extraction (XBOW benchmark) */
  debug?: boolean;
}

export async function runExploitAgent(
  memory: AgentMemory,
  onProgress?: ProgressCallback,
  options?: ExploitAgentOptions
): Promise<AgentResult<ExploitFindings>> {
  const startTime = Date.now();
  const executionMode = memory.context.executionMode || "safe";
  let stopReason: AevRunStopReason = "completed";
  let turnsCompleted = 0;

  const telemetry = new AevTelemetryRecorder({
    evaluationId: memory.context.evaluationId,
    organizationId: memory.context.organizationId || "unknown",
    runType: "exploit_agent",
    executionMode,
  });
  void telemetry.start();

  onProgress?.("exploit", 30, "Initializing agentic exploit analysis...");

  // ── Build prompts ──────────────────────────────────────────────────

  const reconContext = memory.recon
    ? `
Previous Recon Findings:
- Attack Surface: ${memory.recon.attackSurface.join(", ")}
- Entry Points: ${memory.recon.entryPoints.join(", ")}
- API Endpoints: ${memory.recon.apiEndpoints.join(", ")}
- Auth Mechanisms: ${memory.recon.authMechanisms.join(", ")}
- Technologies: ${memory.recon.technologies.join(", ")}
- Potential Vulnerabilities: ${memory.recon.potentialVulnerabilities.join(", ")}
`
    : "";

  // Structured external recon data from real scanning (ports, SSL, HTTP fingerprint, auth surface)
  const externalReconContext = memory.externalRecon
    ? formatExternalRecon(memory.externalRecon)
    : "";

  // Attack plan from plan agent — prioritized chains with turn budgets
  const planContext = memory.plan && memory.plan.prioritizedChains.length > 0
    ? `
ATTACK PLAN (follow this order, allocate turns per budget):
${memory.plan.prioritizedChains.map(c =>
  `${c.rank}. [${c.mitreId}] ${c.attackVector} → ${c.targetEndpoint} (${c.technique}, confidence ${c.confidence}%, ${c.turnBudget} turns) — ${c.rationale}`
).join("\n")}
Total turn budget: ${memory.plan.totalTurnBudget}
${memory.plan.skippedVectors.length > 0 ? `Skipped (low value): ${memory.plan.skippedVectors.join(", ")}` : ""}
Test highest-priority chains first. If confirmed, move to chaining/escalation rather than retesting.`
    : "";

  const adversaryContext = memory.context.adversaryProfile
    ? generateAdversaryPromptContext(memory.context.adversaryProfile)
    : "";

  const policyContext = memory.context.policyContext || "";
  const executionModeConstraints = formatExecutionModeConstraints(executionMode);

  const groundTruthContext = memory.groundTruth
    ? [buildNetworkGroundTruth(memory.groundTruth), buildReconGroundTruth(memory.groundTruth)]
        .filter(Boolean)
        .join("\n\n")
    : "";

  const systemPrompt = `You are the EXPLOIT AGENT, a specialized AI exploitation analysis system for OdinForge AI.

You have access to real security testing tools. Your workflow:
1. FINGERPRINT the target (http_fingerprint) — discover endpoints, forms, parameters, tech stack
2. TEST PAYLOADS (test_payloads) — for EVERY discovered parameter, call test_payloads with all relevant vuln categories. This tool automatically fires dozens of payloads (SSTI, SQLi, CMDi, path traversal, etc.) and reports which ones produce interesting responses. This is your PRIMARY exploitation tool.
3. FOLLOW UP (send_http_request) — if test_payloads reports hits, use send_http_request for targeted follow-up exploitation based on the results.
4. After exploiting, produce your final JSON analysis.

TOOL PRIORITY:
- PREFER test_payloads over send_http_request for initial exploitation. test_payloads fires all standard payloads automatically — no guessing required.
- Use send_http_request only for custom/targeted follow-up requests after test_payloads reveals what works.
- Call test_payloads once per (endpoint, parameter) pair with all relevant vuln_categories in one call.

CRITICAL: Do NOT stop after detecting a vulnerability. You MUST call test_payloads or send_http_request to attempt actual exploitation and data extraction. Detection without exploitation is incomplete.

RULES:
- You are in ${executionMode} mode. Some tools may be blocked — adapt accordingly.
- Do NOT test the same endpoint + vulnerability type twice.
- When http_fingerprint discovers endpoints with parameters, IMMEDIATELY call test_payloads on each parameter.
- Look for flags (FLAG{...}), secrets, tokens, credentials, or sensitive data in every response body.
- If active tools are blocked (safe mode), use passive tools (http_fingerprint, port_scan, check_ssl_tls, run_protocol_probe) and produce analytical assessment augmented with passive recon data.
- Think like an offensive security expert. Be realistic about success likelihood.
- Maximize the value of each tool call — don't waste turns on redundant probes.
${adversaryContext}
${executionModeConstraints}
${policyContext}`;

  const userPrompt = `Analyze exploitability for this security exposure:

Asset ID: ${memory.context.assetId}
Exposure Type: ${memory.context.exposureType}
Priority: ${memory.context.priority}
Description: ${memory.context.description}
${reconContext}
${externalReconContext ? `\n${externalReconContext}\n` : ""}${groundTruthContext ? `\n${groundTruthContext}\n` : ""}${planContext ? `\n${planContext}\n` : ""}
Begin by calling the most relevant tool(s) to validate the highest-risk attack vectors, then analyze results and continue probing as needed.`;

  const finalTurnInstruction = `You have finished your investigation. Now produce your final exploitation analysis as a JSON object with this exact structure (nothing else):
{
  "exploitable": boolean,
  "exploitChains": [
    {
      "name": "Name of the exploit chain",
      "technique": "MITRE ATT&CK technique ID (e.g., T1190)",
      "description": "Detailed description including evidence from tool results",
      "success_likelihood": "high" | "medium" | "low"
    }
  ],
  "cveReferences": ["CVE-XXXX-XXXX if applicable"],
  "misconfigurations": ["list of exploitable misconfigurations"]
}

CRITICAL RULES:
- If ANY tool returned vulnerable=true or confidence>=50, you MUST include at least one exploit chain. Set exploitable=true.
- Every confirmed vulnerability from tool results MUST appear as an exploit chain with the specific vulnerability type in the name (e.g., "SQL Injection via email parameter", "XSS in comment field", "Path Traversal via null byte").
- Include the tool name and key evidence details in each chain's description.
- Do not omit findings just because you didn't have time to fully chain them. A single confirmed vulnerability is still an exploit chain.`;

  // ── Agentic loop ───────────────────────────────────────────────────

  const router = createExploitModelRouter();
  const toolCtx: ExploitToolContext = {
    executionMode,
    organizationId: memory.context.organizationId,
    evaluationId: memory.context.evaluationId,
    assetId: memory.context.assetId,
  };

  const messages: OpenAI.ChatCompletionMessageParam[] = [
    { role: "system", content: systemPrompt },
    { role: "user", content: userPrompt },
  ];

  const allEvidence: ToolCallEvidence[] = [];
  const toolCallLog: NonNullable<ExploitFindings["toolCallLog"]> = [];
  let exploitNudgeCount = 0;
  let forceToolChoice: string | null = null;

  try {
    for (let turn = 0; turn < MAX_TURNS; turn++) {
      // Timeout guard
      if (Date.now() - startTime > LOOP_TIMEOUT_MS) {
        stopReason = "timeout";
        onProgress?.("exploit", 38, "Timeout approaching — finalizing...");
        break;
      }
      turnsCompleted = turn + 1;

      const isFinalTurn = turn === MAX_TURNS - 1;
      const progressPct = 30 + Math.floor((turn / MAX_TURNS) * 10);

      // Force final answer on last turn
      if (isFinalTurn) {
        messages.push({ role: "system", content: finalTurnInstruction });
      }

      const { client, model } = router.getForTurn(turn);

      onProgress?.(
        "exploit",
        progressPct,
        turn === 0
          ? "Analyzing attack vectors..."
          : `Turn ${turn + 1}: probing targets...`
      );

      // Determine tool_choice: forced after nudge, auto otherwise
      const currentToolChoice = isFinalTurn
        ? undefined
        : forceToolChoice
          ? { type: "function" as const, function: { name: forceToolChoice } }
          : ("auto" as const);
      forceToolChoice = null; // reset after use

      const llmStart = Date.now();
      const response = await withRateLimitRetry(() =>
        client.chat.completions.create({
          model,
          messages,
          tools: isFinalTurn ? undefined : EXPLOIT_AGENT_TOOLS,
          tool_choice: currentToolChoice,
          max_completion_tokens: 2048,
        })
      );
      const llmDurationMs = Date.now() - llmStart;

      const choice = response.choices[0];
      if (!choice) {
        void telemetry.recordLlmTurn({ turn, model, hadToolCalls: false, toolCallCount: 0, durationMs: llmDurationMs, failureCode: "llm_no_response" });
        throw new Error("No response from Exploit Agent");
      }

      const assistantMsg = choice.message;
      messages.push(assistantMsg as OpenAI.ChatCompletionMessageParam);

      void telemetry.recordLlmTurn({
        turn,
        model,
        hadToolCalls: Boolean(assistantMsg.tool_calls?.length),
        toolCallCount: assistantMsg.tool_calls?.length ?? 0,
        durationMs: llmDurationMs,
      });

      // No tool calls → LLM produced text instead of calling tools.
      // If the agent hasn't attempted exploitation yet (no send_http_request calls)
      // and we have turn budget remaining, nudge it to exploit before giving up.
      if (!assistantMsg.tool_calls || assistantMsg.tool_calls.length === 0) {
        const hasExploited = toolCallLog.some(t => t.toolName === "test_payloads" || t.toolName === "send_http_request");
        const hasUsedTools = toolCallLog.length >= 2; // agent has done recon/validation work
        const hasRemainingBudget = turn < MAX_TURNS - 2; // leave room for exploit + final

        if (!hasExploited && hasUsedTools && hasRemainingBudget && exploitNudgeCount < 2) {
          exploitNudgeCount++;
          // Nudge: push agent back into tool-calling mode AND force test_payloads on next turn
          const vulnSummary = toolCallLog
            .filter(t => t.confidence > 0 || t.toolName === "http_fingerprint")
            .map(t => `${t.toolName}: ${t.resultSummary.slice(0, 200)}`)
            .join("\n") || "See previous tool results above";

          // Extract discovered endpoints for the nudge
          const fpResults = toolCallLog.filter(t => t.toolName === "http_fingerprint");
          let discoveredEndpoints = "";
          for (const fp of fpResults) {
            try {
              const data = JSON.parse(fp.resultSummary);
              if (data.discoveredLinks) discoveredEndpoints += `Discovered links: ${data.discoveredLinks.join(", ")}\n`;
              if (data.discoveredForms) discoveredEndpoints += `Discovered forms: ${JSON.stringify(data.discoveredForms)}\n`;
            } catch {
              discoveredEndpoints += `Fingerprint: ${fp.resultSummary.slice(0, 200)}\n`;
            }
          }

          messages.push({
            role: "system",
            content: `STOP. You have NOT used test_payloads yet. Your previous findings:\n${vulnSummary}\n${discoveredEndpoints}\nYou MUST now call test_payloads to automatically test all payload categories against discovered parameters. Pick the most promising endpoint and parameter, then call test_payloads with vuln_categories ["ssti", "sqli", "cmdi", "path_traversal"]. The tool fires dozens of payloads automatically and reports hits.

Example: test_payloads({ url: "http://target/greet/", parameter: "name", parameter_location: "query", vuln_categories: ["ssti", "sqli", "cmdi"] })

Do NOT manually craft payloads with send_http_request. Use test_payloads — it's faster and more thorough.`,
          });
          forceToolChoice = "test_payloads"; // Force the LLM to call test_payloads
          continue; // Don't break — let the agent try again with forced tool choice
        }
        break;
      }

      // Execute tool calls
      for (const toolCall of assistantMsg.tool_calls) {
        if (toolCall.type !== "function") continue;
        const fnName = toolCall.function.name;
        let fnArgs: Record<string, unknown> = {};
        try {
          fnArgs = JSON.parse(toolCall.function.arguments);
        } catch {
          // Malformed args — tell the model
          messages.push({
            role: "tool",
            tool_call_id: toolCall.id,
            content: JSON.stringify({ error: "Invalid JSON arguments" }),
          });
          continue;
        }

        onProgress?.(
          "exploit",
          progressPct + 1,
          `Executing ${fnName}...`
        );

        const { result, evidence } = await executeExploitTool(fnName, fnArgs, toolCtx);

        messages.push({
          role: "tool",
          tool_call_id: toolCall.id,
          content: result,
        });

        if (evidence) {
          allEvidence.push(evidence);
          toolCallLog.push({
            turn,
            toolName: evidence.toolName,
            arguments: evidence.arguments,
            resultSummary: evidence.resultSummary,
            vulnerable: evidence.vulnerable,
            confidence: evidence.confidence,
            executionTimeMs: evidence.executionTimeMs,
          });
          void telemetry.recordToolCall({
            turn,
            toolName: evidence.toolName,
            arguments: evidence.arguments,
            resultSummary: evidence.resultSummary,
            vulnerable: evidence.vulnerable,
            confidence: evidence.confidence,
            executionTimeMs: evidence.executionTimeMs,
          });
        } else {
          void telemetry.recordToolCall({
            turn,
            toolName: fnName,
            arguments: fnArgs,
            resultSummary: "no_evidence",
            vulnerable: false,
            confidence: 0,
            executionTimeMs: 0,
            failureCode: "tool_parse_error",
          });
        }
      }
    }

    // Detect max turns
    if (turnsCompleted >= MAX_TURNS && stopReason === "completed") {
      stopReason = "max_turns_reached";
    }

    // Force a final JSON-producing call if the last message isn't valid findings JSON.
    // This handles: (a) loop ended on a tool result, (b) loop ended on assistant
    // text that's a natural-language analysis rather than structured JSON.
    const lastMessage = messages[messages.length - 1];
    const lastIsToolResult = !lastMessage || (lastMessage as any).role === "tool";
    const lastIsAssistantWithPendingTools =
      (lastMessage as any)?.role === "assistant" && (lastMessage as any).tool_calls?.length > 0;
    const lastIsNonJsonAssistant = (() => {
      if ((lastMessage as any)?.role !== "assistant") return false;
      const content = (lastMessage as any).content;
      if (typeof content !== "string" || !content.trim()) return true;
      // Check if it parses as valid findings JSON
      try {
        const jsonMatch = content.match(/```(?:json)?\s*([\s\S]*?)```/) || [null, content];
        const parsed = JSON.parse(jsonMatch[1]!.trim());
        return !parsed || typeof parsed.exploitable === "undefined";
      } catch {
        return true; // Not JSON — needs final call
      }
    })();

    const needsFinalCall = lastIsToolResult || lastIsAssistantWithPendingTools || lastIsNonJsonAssistant;

    if (needsFinalCall && Date.now() - startTime < LOOP_TIMEOUT_MS) {
      messages.push({ role: "system", content: finalTurnInstruction });
      const { client, model } = router.getForTurn();
      const finalResponse = await withRateLimitRetry(() =>
        client.chat.completions.create({
          model,
          messages,
          max_completion_tokens: 2048,
        })
      );
      const finalMsg = finalResponse.choices[0]?.message;
      if (finalMsg) {
        messages.push(finalMsg as OpenAI.ChatCompletionMessageParam);
      }
    }

    onProgress?.("exploit", 40, "Parsing exploit findings...");

    // ── Extract findings from last assistant message ──────────────────

    const findings = extractFindings(messages);
    const enriched = enrichWithEvidence(findings, allEvidence, toolCallLog);

    // Attach raw messages for XBOW benchmark flag extraction
    if (options?.debug) {
      (enriched as any)._debugMessages = messages.map((m: any) => ({
        role: m.role,
        content: typeof m.content === "string" ? m.content : null,
      }));
    }

    const bestConfidence = toolCallLog.reduce((max, tc) => Math.max(max, tc.confidence), 0);

    void telemetry.finish({
      stopReason,
      exploitable: enriched.exploitable,
      overallConfidence: bestConfidence,
      findingCount: enriched.exploitChains.length,
      totalTurns: turnsCompleted,
      totalToolCalls: toolCallLog.length,
    });

    return {
      success: true,
      findings: enriched,
      agentName: "Exploit Agent",
      processingTime: Date.now() - startTime,
    };
  } catch (error) {
    const errMsg = error instanceof Error ? error.message : String(error);
    const failureCode = classifyExploitFailure(error);
    void telemetry.recordFailure(failureCode, "exploit_agent", errMsg);
    void telemetry.finish({
      stopReason: "error",
      failureCode,
      errorMessage: errMsg,
      totalTurns: turnsCompleted,
      totalToolCalls: toolCallLog.length,
    });
    throw wrapAgentError("Exploit Agent", error);
  }
}

// ── Helpers ────────────────────────────────────────────────────────────

/** Extract ExploitFindings JSON from the last assistant message with content. */
function extractFindings(messages: OpenAI.ChatCompletionMessageParam[]): ExploitFindings {
  // Walk backwards to find last assistant message with text content
  for (let i = messages.length - 1; i >= 0; i--) {
    const msg = messages[i] as any;
    if (msg.role === "assistant" && typeof msg.content === "string" && msg.content.trim()) {
      const content = msg.content.trim();
      // Try to extract JSON from markdown code block or raw JSON
      const jsonMatch = content.match(/```(?:json)?\s*([\s\S]*?)```/) || [null, content];
      try {
        const parsed = JSON.parse(jsonMatch[1]!.trim()) as ExploitFindings;
        return validateFindings(parsed);
      } catch {
        // Try the full content
        try {
          const parsed = JSON.parse(content) as ExploitFindings;
          return validateFindings(parsed);
        } catch {
          // Can't parse — return empty findings
        }
      }
    }
  }

  return {
    exploitable: false,
    exploitChains: [],
    cveReferences: [],
    misconfigurations: [],
  };
}

/** Validate and normalize parsed findings. */
function validateFindings(raw: any): ExploitFindings {
  return {
    exploitable: Boolean(raw.exploitable),
    exploitChains: Array.isArray(raw.exploitChains)
      ? raw.exploitChains.map((chain: any) => ({
          name: String(chain.name || "Unknown"),
          technique: String(chain.technique || "T0000"),
          description: String(chain.description || ""),
          success_likelihood: validateLikelihood(chain.success_likelihood),
        }))
      : [],
    cveReferences: Array.isArray(raw.cveReferences) ? raw.cveReferences.map(String) : [],
    misconfigurations: Array.isArray(raw.misconfigurations) ? raw.misconfigurations.map(String) : [],
  };
}

function validateLikelihood(likelihood: unknown): "high" | "medium" | "low" {
  const valid = ["high", "medium", "low"];
  return valid.includes(String(likelihood)) ? (likelihood as "high" | "medium" | "low") : "medium";
}

/** Match tool evidence to exploit chains and attach validation data. */
function enrichWithEvidence(
  findings: ExploitFindings,
  evidence: ToolCallEvidence[],
  toolCallLog: NonNullable<ExploitFindings["toolCallLog"]>
): ExploitFindings {
  if (evidence.length === 0) {
    return { ...findings, toolCallLog: toolCallLog.length > 0 ? toolCallLog : undefined };
  }

  // Build lookup of confirmed vulnerabilities from evidence
  const confirmedEvidence = evidence.filter((e) => e.vulnerable);

  const enrichedChains = findings.exploitChains.map((chain) => {
    // Try to match evidence to this chain by vuln type keyword or tool name
    const chainText = `${chain.name} ${chain.description} ${chain.technique}`.toLowerCase();
    const matchingEvidence = confirmedEvidence.filter((e) => {
      const toolText = `${e.toolName} ${e.resultSummary} ${JSON.stringify(e.arguments)}`.toLowerCase();
      // Match by vulnerability type keywords
      const vulnTypes = ["sqli", "xss", "ssrf", "command_injection", "path_traversal", "auth_bypass", "fuzz", "ssl", "port", "smtp", "dns"];
      return vulnTypes.some((vt) => chainText.includes(vt) && toolText.includes(vt)) ||
        // Or match by URL/endpoint
        (e.arguments.url && chainText.includes(String(e.arguments.url).toLowerCase()));
    });

    if (matchingEvidence.length > 0) {
      const bestMatch = matchingEvidence.reduce((a, b) => (a.confidence > b.confidence ? a : b));
      return {
        ...chain,
        validated: true,
        validationVerdict: evidenceToVerdict(bestMatch.confidence),
        validationConfidence: bestMatch.confidence,
        evidence: matchingEvidence.map((e) => ({
          toolName: e.toolName,
          summary: e.resultSummary,
          request: e.httpEvidence?.request,
          response: e.httpEvidence?.response,
          timing: e.httpEvidence?.timing,
        })),
      };
    }

    return chain;
  });

  return {
    ...findings,
    exploitChains: enrichedChains,
    toolCallLog: toolCallLog.length > 0 ? toolCallLog : undefined,
  };
}

/** Classify an exploit agent error into an AevFailureCode. */
function classifyExploitFailure(error: unknown): import("@shared/schema").AevFailureCode {
  const msg = error instanceof Error ? error.message : String(error);
  if (msg.includes("rate limit") || msg.includes("429")) return "llm_rate_limit";
  if (msg.includes("context") && msg.includes("exceed")) return "llm_context_exceeded";
  if (msg.includes("No response")) return "llm_no_response";
  if (msg.includes("JSON") || msg.includes("parse")) return "llm_malformed_json";
  if (msg.includes("timeout") || msg.includes("ETIMEDOUT")) return "tool_timeout";
  if (msg.includes("ECONNREFUSED") || msg.includes("ENOTFOUND")) return "tool_network_error";
  if (msg.includes("circuit breaker")) return "circuit_breaker_open";
  return "none";
}

function evidenceToVerdict(confidence: number): "confirmed" | "likely" | "theoretical" | "false_positive" {
  if (confidence >= 80) return "confirmed";
  if (confidence >= 50) return "likely";
  if (confidence >= 20) return "theoretical";
  return "false_positive";
}

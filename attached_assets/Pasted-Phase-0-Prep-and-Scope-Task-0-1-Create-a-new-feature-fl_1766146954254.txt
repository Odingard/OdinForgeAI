Phase 0 — Prep and Scope

Task 0.1 — Create a new feature flag
	•	Add a backend feature flag: REPORTS_V2_NARRATIVE=true
	•	Default OFF in production until validated
	•	Allow per-tenant override (optional)

Task 0.2 — Define “Report V2” as a new version (do not overwrite V1)
	•	Add report_version enum: v1_template | v2_narrative
	•	Ensure existing reports still generate with v1_template

⸻

Phase 1 — Create the Engagement Narrative Object (ENO)

Task 1.1 — Define ENO schema (server-side)

Create a TypeScript schema file:

File: server/src/reportsV2/eno.schema.ts
	•	Use Zod validation
	•	Fields (minimum required):
	•	engagement_overview
	•	attack_story
	•	business_impact_analysis
	•	defensive_gaps
	•	risk_prioritization_logic
	•	overall_assessment

Acceptance criteria
	•	ENO validates reliably
	•	ENO includes confidence fields (0–1) where applicable

Task 1.2 — Add ENO persistence model

Add a DB table:

Table: report_narratives
	•	id (uuid)
	•	tenant_id
	•	evaluation_id (or report_scope_id if report can cover multiple evals)
	•	report_version (v2_narrative)
	•	eno_json (JSONB)
	•	model_meta (model name, prompt hash, created_at)
	•	created_by (user/system)
	•	created_at

Acceptance criteria
	•	Can store and retrieve ENO for a given evaluation/report

⸻

Phase 2 — Build the Narrative Engine (AI Prompt Architecture)

Task 2.1 — Implement Narrative Engine orchestrator

File: server/src/reportsV2/narrativeEngine.ts

Create a pipeline:
	1.	generateENO()
	2.	generateExecutiveReport()
	3.	generateTechnicalReport()
	4.	generateComplianceReport()
	5.	generateEvidencePackage()

Each stage:
	•	Input: ENO + raw data relevant to stage
	•	Output: section JSON + narrative text blocks

Acceptance criteria
	•	All stages run sequentially
	•	Failures return clear errors and do not corrupt existing reports

Task 2.2 — Add Prompt Packs (role-based prompts)

Create prompt files for each persona:

Folder: server/src/reportsV2/prompts/
	•	leadPentester.system.md
	•	executiveAdvisor.system.md
	•	seniorEngineer.system.md
	•	complianceAssessor.system.md
	•	incidentDoc.system.md

Also create per-stage user prompt templates:
	•	eno.user.md
	•	executive.user.md
	•	technical.user.md
	•	compliance.user.md
	•	evidence.user.md

Acceptance criteria
	•	Prompts are editable without code changes
	•	Prompt pack includes explicit “do not template” rules

Task 2.3 — Add AI output constraints (anti-template)

For each prompt, enforce:
	•	“No bullet-only output”
	•	“Explain reasoning and tradeoffs”
	•	“Avoid generic phrases like ‘overall risk is high due to…’”
	•	“Anchor statements to evidence and findings”
	•	“Write like a human consultant”

Acceptance criteria
	•	Generated prose reads authored and situation-specific

⸻

Phase 3 — Data Inputs (What the AI needs)

Task 3.1 — Build a single “Report Input Builder”

File: server/src/reportsV2/reportInputBuilder.ts

This creates the structured input payload for AI:
	•	Evaluation metadata (tenant, scope, targets, time range)
	•	Findings list (severity, exploitability, exposure type, affected assets)
	•	Attack paths (steps, complexity, TtC)
	•	Evidence artifacts (http captures, traces, logs, timestamps)
	•	Risk scoring output (impact, blast radius, financial estimates)
	•	Compliance mappings (framework controls → findings/evidence)
	•	“Customer context” (industry, crown jewels, environment type) if available

Acceptance criteria
	•	One function returns a single canonical input object
	•	Supports single-evaluation and multi-evaluation report scopes

Task 3.2 — Add “Customer Context” fields (optional but recommended)

If not already stored, add lightweight tenant context fields:
	•	industry
	•	primary_data_types (PII/PCI/PHI/etc.)
	•	critical_systems (billing, auth, production, etc.)
	•	risk_tolerance (low/med/high)

Acceptance criteria
	•	If context exists, it’s injected into ENO generation
	•	If not, the AI uses safe assumptions and labels them as assumptions

⸻

Phase 4 — Report Output Format (True Pentest Report)

Task 4.1 — Define Report V2 section schemas

File: server/src/reportsV2/reportV2.schema.ts

Outputs should include:

Executive
	•	executive_summary (narrative)
	•	top_risks_ranked_by_business_impact
	•	attack_story_summary
	•	financial_exposure
	•	strategic_recommendations
	•	30/60/90_day_plan

Technical
	•	attack_narrative_detailed
	•	findings (with evidence references)
	•	attack_paths_with_reasoning
	•	prioritized_fix_plan (with “why”)
	•	verification_steps

Compliance
	•	framework_summary
	•	control_failures_with_operational_explanations
	•	evidence_links
	•	audit_readiness_notes

Evidence Package
	•	timeline_narrative
	•	artifact_index
	•	what_each_artifact_proves

Acceptance criteria
	•	Output format is consistent and renderable in UI/PDF

Task 4.2 — Persist Report V2 output

Add DB table:

Table: reports
	•	id
	•	tenant_id
	•	scope_id / evaluation_id
	•	report_version
	•	report_json (JSONB)
	•	generated_at
	•	generated_by
	•	status (draft/final)

Acceptance criteria
	•	V2 report is stored and retrievable
	•	V1 and V2 can coexist

⸻

Phase 5 — API Endpoints

Task 5.1 — Add Report V2 generation endpoint

POST /api/reports/v2/generate
Body:
	•	evaluation_id (or scope)
	•	report_types: ["executive","technical","compliance","evidence"]
	•	report_version: "v2_narrative"

Returns:
	•	report_id
	•	status
	•	sections_generated

Task 5.2 — Add Report V2 fetch endpoint

GET /api/reports/v2/:report_id

Task 5.3 — Add “Regenerate with context” endpoint (optional)

POST /api/reports/v2/:report_id/regenerate
Body:
	•	updated tenant context
	•	“focus areas” (optional)

Acceptance criteria
	•	Endpoints validated with Zod
	•	RBAC-protected (admin/analyst)

⸻

Phase 6 — UI Updates (Minimal but High Impact)

Task 6.1 — Add “Report Style” selector in UI

On Reports page:
	•	“Standard Report (V1)”
	•	“Pentest Narrative Report (V2)”

Task 6.2 — Add ENO preview (internal only)

In report debug view (admin only):
	•	show ENO JSON
	•	show model meta + prompt hash

Task 6.3 — Add “Human-grade sections”

Render as:
	•	Narrative paragraphs
	•	“Attack story” as a readable sequence
	•	Evidence references as clickable anchors

Acceptance criteria
	•	V2 reads like a real pentest deliverable in UI

⸻

Phase 7 — Quality Controls (Prevent Template Feel)

Task 7.1 — Add “Anti-template lint” checks

Before saving report:
	•	Reject if too repetitive (same phrase repeated)
	•	Reject if missing “reasoning” fields
	•	Reject if no evidence references exist for high/critical findings

Task 7.2 — Add deterministic fallbacks

If AI fails:
	•	Return V1 report or partial V2 with warnings
	•	Never block UI entirely

⸻

Phase 8 — Testing & Acceptance

Task 8.1 — Create test fixtures

Include:
	•	IAM abuse case
	•	Payment flow bypass case
	•	Multi-vector chain case
	•	Compliance-heavy case

Task 8.2 — E2E tests
	•	Generate V2 report from completed evaluation
	•	Verify sections render
	•	Verify evidence link anchors exist
	•	Verify PDF export (if enabled)

Acceptance criteria
	•	V2 report generation works end-to-end
	•	Output is coherent and non-templated
	•	Executive + technical narratives agree with each other